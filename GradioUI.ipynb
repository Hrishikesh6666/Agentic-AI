{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOkFHDY9JbO8Kubr/FrYalt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hrishikesh6666/Agentic-AI/blob/main/GradioUI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradio UI"
      ],
      "metadata": {
        "id": "Ss-3_SJKzyTM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gradio as gr\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "from IPython.display import display, HTML,Markdown\n"
      ],
      "metadata": {
        "id": "qGi05JDAzyCN"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load environment variables in a file called .env\n",
        "# Print the key prefixes to help with any debugging\n",
        "# You can choose whichever providers you like - or all Ollama\n",
        "\n",
        "load_dotenv(override=True)\n",
        "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
        "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
        "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
        "\n",
        "if openai_api_key:\n",
        "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
        "else:\n",
        "    print(\"OpenAI API Key not set\")\n",
        "\n",
        "if anthropic_api_key:\n",
        "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
        "else:\n",
        "    print(\"Anthropic API Key not set\")\n",
        "\n",
        "if google_api_key:\n",
        "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
        "else:\n",
        "    print(\"Google API Key not set\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-bmcCw23Cri",
        "outputId": "5c339801-b1ed-4d87-932b-90d5a59498b2"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI API Key exists and begins sk-proj-\n",
            "Anthropic API Key not set\n",
            "Google API Key exists and begins AIzaSyA1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "openai = OpenAI()\n",
        "\n",
        "gemini_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
        "ollama_url = \"http://localhost:11434/v1\"\n",
        "\n",
        "gemini = OpenAI(api_key=google_api_key,base_url=gemini_url)\n",
        "ollama = OpenAI(api_key=\"ollama\",base_url=ollama_url)"
      ],
      "metadata": {
        "id": "0aVR16wH3JGp"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_message = \"You are a helpful assistant.\"\n",
        "\n",
        "def message_gpt(prompt):\n",
        "    messages = [{\"role\":\"system\", \"content\": system_message},{\"role\":\"user\", \"content\": prompt}]\n",
        "    responses =openai.chat.completions.create(model=\"gpt-4.1-mini\",messages=messages)\n",
        "    return responses.choices[0].message.content"
      ],
      "metadata": {
        "id": "gt-vybRYAYfj"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "message_gpt(\"what is your name?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "dsOXicBNCFi8",
        "outputId": "96da4e0e-a0f8-4fe8-a98c-c66d160f47a1"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I am called ChatGPT. How can I assist you today?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# simple function\n",
        "def shout(text):\n",
        "  print(f\"Shout has been called with input{text}\")\n",
        "  return text.upper()"
      ],
      "metadata": {
        "id": "7Hw0joMOCMEa"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gr.Interface(shout,inputs = \"text\",outputs = \"text\").launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "ZHJZaZ1CC4xq",
        "outputId": "f8662101-c6f4-4b46-bd11-e183233d6bc8"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://f09758fb6484bb2d83.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://f09758fb6484bb2d83.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gr.Interface(shout,inputs = \"text\",outputs = \"text\",flagging_mode=\"never\").launch(inbrowser=True,auth=(\"admin123\",\"Pass@123\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "Iv79UGk9DNay",
        "outputId": "787a74a3-a1e9-4646-f377-e9587bcbc8fa"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://ed46b7eb32232422e7.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://ed46b7eb32232422e7.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "message_input =gr.Textbox(lines=7,label=\"Chat with AI\" ,show_label=\"Your message prompt\")\n",
        "message_output = gr.Textbox(lines=7,label=\"Response\")\n",
        "\n",
        "view = gr.Interface(message_gpt,title=\"GPT\",inputs = message_input,outputs = message_output,flagging_mode=\"never\").launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "IU7dcjwhELeJ",
        "outputId": "cf1028ed-c34c-4aea-eed4-9d92b9aa3371"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://5c169d5437a65c20f9.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://5c169d5437a65c20f9.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create a call that streams back results\n",
        "# If you'd like a refresher on Generators (the \"yield\" keyword),\n",
        "# Please take a look at the Intermediate Python guide in the guides folder\n",
        "\n",
        "from httpx import stream\n",
        "def stream_gpt(prompt):\n",
        "  Messages=[\n",
        "      {\"role\":\"system\",\"content\":system_message},\n",
        "      {\"role\":\"user\",\"content\":prompt}\n",
        "  ]\n",
        "\n",
        "  stream=openai.chat.completions.create(\n",
        "      model=\"gpt-4.1-mini\",\n",
        "      messages=Messages,\n",
        "      stream=True\n",
        "  )\n",
        "  result=\"\"\n",
        "  for response in stream:\n",
        "    result+=response.choices[0].delta.content or \"\"\n",
        "    yield result\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "LQMf4L18E22j"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "message_input = gr.Textbox(label=\"Your message:\", info=\"Enter a message for GPT-4.1-mini\", lines=7)\n",
        "message_output = gr.Markdown(label=\"Response:\")\n",
        "\n",
        "view = gr.Interface(\n",
        "    fn=stream_gpt,\n",
        "    title=\"GPT\",\n",
        "    inputs=[message_input],\n",
        "    outputs=[message_output],\n",
        "    examples=[\n",
        "        \"Explain the Transformer architecture to a layperson\",\n",
        "        \"Explain the Transformer architecture to an aspiring AI engineer\",\n",
        "        ],\n",
        "    flagging_mode=\"never\"\n",
        "    )\n",
        "view.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "umL9J3PBGV5t",
        "outputId": "8a83c35a-a94d-485e-b26f-2fe7327b641b"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://539fc87ecc66eebde8.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://539fc87ecc66eebde8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt update\n",
        "!sudo apt install -y pciutils\n",
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnEfcpTVLse8",
        "outputId": "408dbc8e-2aae-4443-f46e-7ec86941492d"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rHit:1 https://cli.github.com/packages stable InRelease\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com (185.125.190.83)] [Connecting to security.\u001b[0m\r                                                                               \rHit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:5 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Shout has been called with inputhi\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "63 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "pciutils is already the newest version (1:3.7.0-6).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 63 not upgraded.\n",
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading ollama-linux-amd64.tar.zst\n",
            "##################                                                        25.9%Shout has been called with inputhi\n",
            "######################################################################## 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m No NVIDIA/AMD GPU detected. Ollama will run in CPU-only mode.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! ollama pull llama3.2:1b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vclDhd5MA6E",
        "outputId": "2f3d06c1-7f87-444e-8ff9-ba3b5ebc1640"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! ollama pull llama3.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIZVJ-zrPyYc",
        "outputId": "ff161f01-f42f-4afd-dea1-74a0e3f99d44"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "OLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n",
        "\n",
        "ollama = OpenAI(base_url=OLLAMA_BASE_URL, api_key='ollama')"
      ],
      "metadata": {
        "id": "XkJeduV-MdiG"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a fun fact\n",
        "\n",
        "response = ollama.chat.completions.create(model=\"llama3.2:1b\", messages=[{\"role\": \"user\", \"content\": \"Tell me a fun fact\"}])\n",
        "\n",
        "response.choices[0].message.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "4IOzTxyQMqmc",
        "outputId": "f715dd6f-6032-4ff1-a3dd-9d3a38803276"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Here\\'s a fun fact for you: Did you know that there is a type of jellyfish that is immortal? The Turritopsis dohrnii, also known as the \"immortal jellyfish,\" is a species of jellyfish that can transform its body into a younger state through a process called transdifferentiation. This means it can essentially revert back to its polyp stage and grow back into an adult again, making it theoretically immortal.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create a call that streams back results\n",
        "# If you'd like a refresher on Generators (the \"yield\" keyword),\n",
        "# Please take a look at the Intermediate Python guide in the guides folder\n",
        "\n",
        "def stream_ollama(prompt):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "      ]\n",
        "    stream = ollama.chat.completions.create(\n",
        "        #model='llama3.2:1b',\n",
        "        model='llama3.1',\n",
        "        messages=messages,\n",
        "        stream=True\n",
        "    )\n",
        "    result = \"\"\n",
        "    for chunk in stream:\n",
        "        result += chunk.choices[0].delta.content or \"\"\n",
        "        yield result"
      ],
      "metadata": {
        "id": "XaAuLoEbMsXD"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "message_input = gr.Textbox(label=\"Your message:\", info=\"Enter a message for GPT-4.1-mini\", lines=7)\n",
        "message_output = gr.Markdown(label=\"Response:\")\n",
        "\n",
        "view = gr.Interface(\n",
        "    fn=stream_ollama,\n",
        "    title=\"OLLAMA\",\n",
        "    inputs=[message_input],\n",
        "    outputs=[message_output],\n",
        "    examples=[\n",
        "        \"Explain the Transformer architecture to a layperson\",\n",
        "        \"Explain the Transformer architecture to an aspiring AI engineer\",\n",
        "        ],\n",
        "    flagging_mode=\"never\"\n",
        "    )\n",
        "view.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "5Aw__Qw4M67T",
        "outputId": "198ce7e1-feb0-498d-9c71-f906b935e365"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://e027cfe29cc093dfd0.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://e027cfe29cc093dfd0.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "e6d412c0",
        "outputId": "3e238802-70a9-4990-dd4f-701a2301d51f"
      },
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Input and Output Components\n",
        "message_input = gr.Textbox(\n",
        "    label=\"ðŸ’¬ Your Message\",\n",
        "    placeholder=\"Type your question or prompt here...\",\n",
        "    lines=7,\n",
        "    show_copy_button=True\n",
        ")\n",
        "\n",
        "message_output = gr.Markdown(\n",
        "    label=\"ðŸ¤– Response\",\n",
        "    elem_classes=\"response-box\"\n",
        ")\n",
        "\n",
        "# Interface Layout\n",
        "view = gr.Interface(\n",
        "    #fn=stream_ollama,\n",
        "    fn=stream_gpt,\n",
        "    title=\"âœ¨ OLLAMA AI Assistant\",\n",
        "    description=\"Ask me anything â€” from AI concepts to coding help. Iâ€™ll respond with clear, structured answers.\",\n",
        "    inputs=[message_input],\n",
        "    outputs=[message_output],\n",
        "    examples=[\n",
        "        \"Explain the Transformer architecture to a layperson\",\n",
        "        \"Explain the Transformer architecture to an aspiring AI engineer\",\n",
        "        \"Write a SQL query to find the top 5 highest-paid employees\",\n",
        "        \"Generate a Python script to create a QR code\"\n",
        "    ],\n",
        "    theme=\"soft\",  # Optional: try \"default\", \"glass\", \"soft\", \"monochrome\"\n",
        "    flagging_mode=\"never\"\n",
        ")\n",
        "\n",
        "# Launch with custom settings\n",
        "view.launch(\n",
        "    share=True,  # allows public link\n",
        "    show_error=True\n",
        ")"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://c37453c757ca5dcd07.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://c37453c757ca5dcd07.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def stream_model(prompt, model):\n",
        "  if model==\"GPT\":\n",
        "    result= stream_gpt(prompt)\n",
        "  elif model==\"OLLAMA\":\n",
        "    result=stream_ollama(prompt)\n",
        "  yield from result"
      ],
      "metadata": {
        "id": "YEdK2NUdfb5_"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Input Components\n",
        "message_input = gr.Textbox(\n",
        "    label=\"ðŸ’¬ Your Message\",\n",
        "    placeholder=\"Type your question or prompt here...\",\n",
        "    lines=7,\n",
        "    show_copy_button=True\n",
        ")\n",
        "\n",
        "model_selector = gr.Dropdown(\n",
        "    [\"GPT\", \"Ollama\"],\n",
        "    label=\"ðŸ¤– Select Model\",\n",
        "    value=\"GPT\",\n",
        "    interactive=True\n",
        ")\n",
        "\n",
        "# Output Component\n",
        "message_output = gr.Markdown(\n",
        "    label=\"ðŸ“œ Response\",\n",
        "    elem_classes=\"response-box\"\n",
        ")\n",
        "\n",
        "# Interface Layout\n",
        "view = gr.Interface(\n",
        "    fn=stream_model,\n",
        "    title=\"âœ¨ LLM Playground\",\n",
        "    description=\"Choose your model and ask anything â€” from AI concepts to coding help. Responses are clear, structured, and tailored to your input.\",\n",
        "    inputs=[message_input, model_selector],\n",
        "    outputs=[message_output],\n",
        "    examples=[\n",
        "        [\"Explain the Transformer architecture to a layperson\", \"GPT\"],\n",
        "        [\"Explain the Transformer architecture to an aspiring AI engineer\", \"Claude\"],\n",
        "        [\"Write a SQL query to find the top 5 highest-paid employees\", \"GPT\"],\n",
        "        [\"Generate a Python script to create a QR code\", \"Claude\"]\n",
        "    ],\n",
        "    theme=\"glass\",  # Options: \"default\", \"soft\", \"glass\", \"monochrome\"\n",
        "    flagging_mode=\"never\"\n",
        ")\n",
        "\n",
        "# Launch Settings\n",
        "view.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "k-YEHDpEfzDf",
        "outputId": "f2a2505d-d7fe-4f14-db49-8f7818cdf861"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gradio/components/dropdown.py:230: UserWarning: The value passed into gr.Dropdown() is not in the list of choices. Please update the list of choices to include: Claude or set allow_custom_value=True.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://0471914ae2e2c75b47.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://0471914ae2e2c75b47.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sc-9wUF3gT6z"
      },
      "execution_count": 115,
      "outputs": []
    }
  ]
}